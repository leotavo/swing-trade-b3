# Release: v0.5.0 — Issue #5 concluída (Dataset processado)

- Status: publicado
- Data: 2025-09-01
- Escopo: Implementar rotina de limpeza/normalização e persistência do dataset processado (Milestone 2, Issue #5) e expor CLI de processamento.

## Resumo

- Schema do dataset processado documentado (`docs/data-schema.md`).
- Funções:
  - `clean_and_validate(df_raw)` — normaliza tipos (UTC, dtypes), remove inválidos/duplicatas, ordena por `symbol,date`.
  - `save_processed(symbol, df, fmt="parquet|csv", compression=...)` — persiste idempotente em `data/processed/`.
  - `load_raw(symbol)` — carrega partições de `data/raw/` (CSV/Parquet) para pipeline de processamento.
- CLI `process`: `python -m app process -s <SYMS> [--start ... --end ...] --format parquet --compression snappy`.
- Observabilidade: suporte a `--log-json` (logs estruturados) e `--json-summary` (no `fetch`).

## Evidências

- Testes: `pytest -q` → 9 passed.
- Execução exemplo:
  - `python -m app process --symbol PETR4 --start 2023-01-01 --end 2024-01-01 --format parquet --compression snappy` → `data/processed/PETR4.parquet`.

## Arquivos principais

- Código: `src/app/processing.py`, `src/app/__main__.py` (subcomando process).
- Docs: `docs/data-schema.md`, README (seções de Processamento e Pipeline ponta a ponta).
- Testes: `tests/test_data_cleaning.py`.

## Notas

- Processamento idempotente; compressão Parquet recomendada (`snappy`).
- `.gitattributes` adicionado para normalizar finais de linha (LF) no repositório.
